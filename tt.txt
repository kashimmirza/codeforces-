1.



.general training and knowledge distillation training, a custom 8â€layered
Convolutional Neural Network model was used as a feature extractor
In all the
training procedure, total data was divided into training, testing and validation group as perthe ration
80:10:10. All experiments was performed o
In order to optimize and to obtain better performances, data augmentation, without
augmentation, classification using SVM and KNN was performed. For the augmentation experiment,
augmentation as per the Table 1 was applied first. While performing 64 Ã— 64 augmentation
experiment, original image was resized to (64, 64) before applying augmentation where
augmentation was performed after resizing original image into (32, 32) for the experiment of 32 Ã— 32
augmentation. In both experiments, data was fed into the model according to batch size 32. For the
without augmentation training, plain data was fed into the model according to batch size 64 after
resizing the original image to (64, 64) or (32, 32) as per the experiment requirement. Weights and
biases of all trainable parameters were randomly initialized at the beginning of training. To optimize
the weights and biases, Stochastic Gradient Descent (SGD) [63] optimizer was used while performing
the forward propagation. The learning rate was set to 0.001 with a momentum of 0.9. While
performing backâ€propagation to calculate the error and update the weights and biases, cross entropy,
shown in Equation (9), was used as a loss function. Predicted result and combined validation loss
and accuracy of augmentation and without augmentation experiment for both image sizes are
depicted in    Figure 5;    Figure 6, respectively.
ğ¿ğ‘œğ‘ ğ‘  àµŒ àµ†áˆºğ‘¦ ğ‘™ğ‘œğ‘”áˆºğ‘áˆ» àµ… áˆº1 àµ† ğ‘¦áˆ» logáˆº1 àµ† ğ‘áˆ»áˆ» (9)
In the experiment of CNNâ€SVM and CNNâ€KNN, a CNN model was used to extract the features.
Later these features were classified through SVM or KNN as per the objective of our experiment. The
outline of this architecture is depicted


 knowledge distillation: a pretrained 146 layers custom ResNet model [70] was taken as a teacher
model, whereas a custom 8 layers CNN model (showed in Figure 4) acted as a student model. In
order to mimic the teacher model, initially using the pretrained teacher model, the softmax value of
training images was recorded and also the training label was predicted. Later, the softmax value of
training images was swapped if the teacher model misâ€predicts the training label. This corrected
softmax value was used as a training label while training student model
